{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype explanations\n",
    "\n",
    "This script generates the typical prototype explanations for a prediction.\n",
    "\n",
    "1) make sure to run the explanation script to generate a json file containing the nearest images in the training set for each prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import skimage as ski\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from quanproto.augmentation import enums\n",
    "from quanproto.datasets import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from quanproto.models import helper\n",
    "from quanproto.models import receptive_field\n",
    "from quanproto.metrics import helpers\n",
    "\n",
    "from quanproto.utils.explanations import load_model\n",
    "from quanproto.utils.evaluation import get_experiment_info\n",
    "from quanproto.utils.evaluation import get_experiment_results\n",
    "\n",
    "import quanproto.utils.dataloader as qf\n",
    "import quanproto.datasets.functional as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = os.environ[\"USER\"]\n",
    "\n",
    "experiment_config = {\n",
    "    \"experiment_dir\": f\"/home/{USER}/repos/QuanProto/experiments/PIPNet\",\n",
    "    \"dataset_dir\": f\"/home/{USER}/data/quanproto\",\n",
    "    \"model\": \"pipnet\",\n",
    "    \"dataset\": \"cub200\",\n",
    "    \"augmentation\": \"geometric_photometric\",\n",
    "    \"feature\": \"resnet50\",\n",
    "    \"fold\": 0,\n",
    "    \"run\": \"chocolate-hill-3\",\n",
    "    \"explanation\": \"prp\",\n",
    "}\n",
    "\n",
    "RESULT_DIR = f\"/home/{USER}/repos/Quanproto/example_maps\"\n",
    "\n",
    "k = 3 # number of top k prototypes to explain the image\n",
    "test_image_idx = 2 # index of the image to explain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_info = get_experiment_info(experiment_config)\n",
    "\n",
    "dataset = sf.get_dataset(experiment_config[\"dataset_dir\"], experiment_config[\"dataset\"])\n",
    "\n",
    "train_data_info = dataset.fold_info(experiment_config['fold'], \"train\")\n",
    "train_root_dir = dataset.fold_dirs(experiment_config['fold'])[\"train\"]\n",
    "\n",
    "test_data_info = dataset.test_info()\n",
    "test_root_dir = dataset.test_dirs()[\"test\"]\n",
    "\n",
    "fold_info = experiment_info[f\"fold_{experiment_config['fold']}\"]\n",
    "run_info = fold_info[experiment_config[\"run\"]]\n",
    "\n",
    "try:\n",
    "    # read the config json file\n",
    "    with open(run_info[\"config\"], \"r\") as f:\n",
    "        config = json.load(f)\n",
    "except KeyError:\n",
    "    raise FileNotFoundError(f\"Could not find config file for run {experiment_config['run']}\")\n",
    "\n",
    "model = load_model(\n",
    "    experiment_config[\"model\"],\n",
    "    experiment_config[\"explanation\"],\n",
    "    config,\n",
    "    dataset.num_classes(),\n",
    "    dataset.multi_label(),\n",
    "    run_info[\"state_dict\"],\n",
    ")\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# 1 = prototype spatial size like 1x1, 224 = input image size like 224x224\n",
    "proto_layer_rf_info = receptive_field.prototype_receptive_field(model.backbone, 1, 224)\n",
    "\n",
    "\n",
    "technique = \"topk_prototype_images\"\n",
    "experiment_results = get_experiment_results(\n",
    "    experiment_config, technique=technique\n",
    ")\n",
    "fold_info = experiment_results[f\"fold_{experiment_config['fold']}\"]\n",
    "run_info = fold_info[experiment_config[\"run\"]]\n",
    "try:\n",
    "    # read the config json file\n",
    "    with open(run_info[technique], \"r\") as f:\n",
    "        topk_prototype_images = json.load(f)\n",
    "except KeyError:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find config file for run {experiment_config['run']}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN: tuple[float, float, float] = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD: tuple[float, float, float] = (0.229, 0.224, 0.225)\n",
    "transform = [\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ToTensorV2(),\n",
    "]\n",
    "\n",
    "train_bboxes = None\n",
    "test_bboxes = None\n",
    "if \"bboxes\" in train_data_info and \"bboxes\" in test_data_info:\n",
    "    train_bboxes = train_data_info[\"bboxes\"]\n",
    "    test_bboxes = test_data_info[\"bboxes\"]\n",
    "\n",
    "    transform = A.Compose(enums.AugmentationPipelines[\"crop_resize\"] + transform)\n",
    "else:\n",
    "    transform = A.Compose(enums.AugmentationPipelines[\"resize\"] + transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(root, data_info, idx):\n",
    "    # get the first batch\n",
    "    img = ski.io.imread(os.path.join(root, data_info[\"paths\"][idx]))\n",
    "    if len(img.shape) == 2:\n",
    "        # convert to 3 channels\n",
    "        img = ski.color.gray2rgb(img)\n",
    "    if img.shape[2] == 4:\n",
    "        img = ski.color.rgba2rgb(img)\n",
    "    if img.shape[2] == 2:\n",
    "        raise ValueError(\"Image has 2 channels\")\n",
    "\n",
    "    if \"bboxes\" in data_info:\n",
    "        largest_bbox = F.combine_bounding_boxes(data_info[\"bboxes\"][idx])\n",
    "        img = transform(image=img, cropping_bbox=largest_bbox)[\"image\"]\n",
    "    else:\n",
    "        img = transform(image=img)[\"image\"]\n",
    "\n",
    "    # check what dtype is returned\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.float()\n",
    "    else:\n",
    "        img = torch.tensor(img).float()\n",
    "    img = img.cuda()\n",
    "    # expand the batch dimension\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = load_img(test_root_dir, test_data_info, test_image_idx)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, similarity_maps, _ = model.explain(test_img)\n",
    "\n",
    "# get the maximum value of the similarity maps\n",
    "similarity_scores = torch.functional.F.max_pool2d(\n",
    "    similarity_maps, kernel_size=similarity_maps.shape[2:]\n",
    ").squeeze().unsqueeze(0)\n",
    "\n",
    "# get the top k similarity scores indices\n",
    "_, topk_indices = torch.topk(similarity_scores, k=k, dim=1)\n",
    "\n",
    "# print(topk_indices[0])\n",
    "# print(similarity_scores[0, topk_indices[0]])\n",
    "\n",
    "# use only the top k similarity maps\n",
    "similarity_maps = torch.stack(\n",
    "    [similarity_maps[i, topk_indices[i]] for i in range(similarity_maps.shape[0])]\n",
    ")\n",
    "proto_rf_info = receptive_field.prototype_rf(similarity_maps, proto_layer_rf_info)\n",
    "saliency_maps = model.saliency_maps(test_img, topk_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_image_explanation(prototype_id):\n",
    "    prototype_info = topk_prototype_images[str(prototype_id)]\n",
    "\n",
    "    train_imgs = torch.empty(0).cuda()\n",
    "    for img_id in prototype_info[\"ids\"]:\n",
    "        img = load_img(train_root_dir, train_data_info, img_id)\n",
    "        train_imgs = torch.cat((train_imgs, img), 0)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        _, similarity_maps, _ = model.explain(train_imgs)\n",
    "\n",
    "    # get the similarity maps from the prototype id\n",
    "    similarity_maps = similarity_maps[:,prototype_id]\n",
    "\n",
    "    proto_id_tensor = torch.tensor([prototype_id]).cuda().repeat(train_imgs.shape[0]).unsqueeze(1)\n",
    "    saliency_maps = model.saliency_maps(train_imgs,proto_id_tensor)\n",
    "\n",
    "    bboxes = []\n",
    "    for i in range(saliency_maps.shape[0]):\n",
    "        bboxes.append(helpers.bounding_box(saliency_maps[i].squeeze()))\n",
    "\n",
    "    return train_imgs, saliency_maps, bboxes, prototype_info[\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_test_explanation(img, saliency_map, similarity_map, proto_rf_bb):\n",
    "    saliency_map_percentile_mask = helpers.percentile_mask(saliency_map)\n",
    "    bb = helpers.bounding_box(saliency_map)\n",
    "    cropped_saliency_map = saliency_map * saliency_map_percentile_mask\n",
    "\n",
    "    # invert the normalization from the dataloader\n",
    "    img = helper.invert_normalize(img).cpu()\n",
    "    # change the channel order to (height, width, channels)\n",
    "    img = img.squeeze(0).permute(1, 2, 0).numpy()\n",
    "\n",
    "    # show original image, cropped saliency map, bbox, proto_rf box, similarity map\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "\n",
    "\n",
    "    # show the original image\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # show the cropped saliency map\n",
    "    axes[1].imshow(img)\n",
    "    axes[1].imshow(cropped_saliency_map.cpu(), alpha=0.5, cmap=\"viridis\")\n",
    "    axes[1].set_title(\"Cropped Saliency Map\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # show the bounding box\n",
    "    lower_y, upper_y, lower_x, upper_x = bb\n",
    "    axes[2].imshow(img)\n",
    "    axes[2].add_patch(\n",
    "        plt.Rectangle(\n",
    "            (lower_x, lower_y),\n",
    "            upper_x - lower_x,\n",
    "            upper_y - lower_y,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "    )\n",
    "    axes[2].set_title(\"Bounding Box\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    # show the prototype receptive field\n",
    "    lower_y, upper_y, lower_x, upper_x = proto_rf_bb\n",
    "    axes[3].imshow(img)\n",
    "    axes[3].add_patch(\n",
    "        plt.Rectangle(\n",
    "            (lower_x +1, lower_y +1),\n",
    "            upper_x - lower_x -3,\n",
    "            upper_y - lower_y -3,\n",
    "            linewidth=2,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "    )\n",
    "    axes[3].set_title(\"Prototype RF\")\n",
    "    axes[3].axis(\"off\")\n",
    "\n",
    "    # show the similarity map\n",
    "    axes[4].imshow(similarity_map.cpu(), cmap=\"viridis\")\n",
    "    axes[4].set_title(\"Similarity Map\")\n",
    "    # axes[4].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_images(train_images, bboxes):\n",
    "    k = train_images.shape[0]\n",
    "\n",
    "    fig, axes = plt.subplots(1, k, figsize=(15, 3))\n",
    "\n",
    "    for i in range(k):\n",
    "        img = helper.invert_normalize(train_images[i]).cpu()\n",
    "        img = img.squeeze(0).permute(1, 2, 0).numpy()\n",
    "\n",
    "        lower_y, upper_y, lower_x, upper_x = bboxes[i]\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].add_patch(\n",
    "            plt.Rectangle(\n",
    "                (lower_x, lower_y),\n",
    "                upper_x - lower_x,\n",
    "                upper_y - lower_y,\n",
    "                linewidth=1,\n",
    "                edgecolor=\"r\",\n",
    "                facecolor=\"none\",\n",
    "            )\n",
    "        )\n",
    "        axes[i].set_title(f\"Train Image {i}\")\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-1 Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Input Reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_test_explanation(test_img, saliency_maps[0][0], similarity_maps[0][0], proto_rf_info[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Train Input Reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, _, bbs, labels = train_image_explanation(topk_indices.tolist()[0][0])\n",
    "print(labels)\n",
    "show_train_images(train_images, bbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-2 Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Input Reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_test_explanation(test_img, saliency_maps[0][1], similarity_maps[0][1], proto_rf_info[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Train Input Reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, _,  bbs, labels = train_image_explanation(topk_indices.tolist()[0][1])\n",
    "print(labels)\n",
    "show_train_images(train_images, bbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-3 Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Input Reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_test_explanation(test_img, saliency_maps[0][2], similarity_maps[0][2], proto_rf_info[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Train Input Reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, _, bbs, labels = train_image_explanation(topk_indices.tolist()[0][2])\n",
    "print(labels)\n",
    "show_train_images(train_images, bbs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
